import requests
from kafka import KafkaProducer, KafkaAdminClient
from kafka.admin import NewTopic
import json
import logging
import time
import os

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Load API Key from environment variables (replace hardcoded API_KEY)
API_KEY = os.getenv('RAPIDAPI_KEY', '61d43a578emsh8c5af2bd4fd2511p1f8765jsn048198f32fe0')

# Your RapidAPI endpoint configuration
BASE_URL = 'https://yahoo-finance15.p.rapidapi.com/api/v1/markets/options/most-active'
KAFKA_TOPIC = 'finance-data'

# Load Kafka servers dynamically from a JSON file generated by Terraform
def load_kafka_brokers():
    try:
        with open("kafka_brokers.json", "r") as f:
            kafka_brokers = json.load(f)["kafka_bootstrap_servers"]["value"]
            return kafka_brokers
    except Exception as e:
        logging.error(f"Error loading Kafka brokers: {e}")
        return None

# Function to create a Kafka topic
def create_kafka_topic(topic_name, kafka_servers, num_partitions=1, replication_factor=1):
    try:
        admin_client = KafkaAdminClient(bootstrap_servers=kafka_servers)
        topic_list = [NewTopic(name=topic_name, num_partitions=num_partitions, replication_factor=replication_factor)]
        admin_client.create_topics(new_topics=topic_list, validate_only=False)
        logging.info(f"Topic '{topic_name}' created successfully.")
    except Exception as e:
        logging.error(f"Error creating topic '{topic_name}': {e}")
        if 'already exists' in str(e):
            logging.info(f"Topic '{topic_name}' already exists. Continuing...")
        else:
            raise

# Function to fetch finance data with pagination
def get_finance_data(page=1):
    try:
        headers = {
            'x-rapidapi-host': 'yahoo-finance15.p.rapidapi.com',
            'x-rapidapi-key': API_KEY
        }
        params = {
            'type': 'STOCKS',
            'page': page,  # Request the current page
        }
        response = requests.get(BASE_URL, headers=headers, params=params)
        response.raise_for_status()  # Raise an HTTPError for bad responses
        
        # Log and return the JSON response data
        data = response.json()
        logging.info(f"Received finance data from page {page}: {json.dumps(data, indent=2)}")
        return data
    except requests.exceptions.RequestException as e:
        logging.error(f"Error fetching finance data on page {page}: {e}")
        return None

# Function to send data to Kafka
def send_to_kafka(producer, topic, data):
    try:
        # Ensure the data is sent as individual records
        if isinstance(data, list):
            logging.info(f"Sending {len(data)} records to Kafka for topic '{topic}'.")
            for record in data:
                logging.info(f"Sending record to Kafka: {json.dumps(record)}")
                producer.send(topic, value=record)
        else:
            logging.info(f"Sending single record to Kafka for topic '{topic}'.")
            producer.send(topic, value=data)
        
        producer.flush()
        logging.info(f"Finance data sent to Kafka topic '{topic}'.")
    except Exception as e:
        logging.error(f"Error sending data to Kafka: {e}")

# Main script
if __name__ == "__main__":
    # Load Kafka servers
    kafka_servers = load_kafka_brokers()
    if not kafka_servers:
        logging.error("Kafka brokers could not be loaded. Exiting.")
        exit(1)

    # Create Kafka topic (if not already created)
    create_kafka_topic(KAFKA_TOPIC, kafka_servers)

    # Initialize Kafka producer
    producer = KafkaProducer(
        bootstrap_servers=kafka_servers.split(","),
        value_serializer=lambda v: json.dumps(v).encode('utf-8'),
        acks='all',  # Ensure all records are acknowledged before moving on
        retries=3,   # Retry sending in case of transient errors
    )
    
    total_records_fetched = 0
    target_records = 1000  # Set the target number of records
    page = 1
    records_per_page = 20  # Since API returns 20 records per page

    while total_records_fetched < target_records:
        finance_data = get_finance_data(page=page)
        
        if finance_data and 'meta' in finance_data:
            total_available = finance_data['meta'].get('total', target_records)  # Fallback to target_records if 'total' is missing
            records_on_page = finance_data['meta'].get('count', records_per_page)
            
            # Extract the actual records from the 'body' key
            records = finance_data.get('body', [])
            if records:
                logging.info(f"Fetched {len(records)} records from page {page}.")
                
                # Send the fetched data to Kafka
                send_to_kafka(producer, KAFKA_TOPIC, records)
                total_records_fetched += len(records)
                logging.info(f"Total records fetched so far: {total_records_fetched}")

            # Stop if there are no more pages to fetch or all records are fetched
            if total_records_fetched >= total_available or records_on_page == 0:
                logging.info(f"Fetched all available records. Stopping at page {page}.")
                break

            # Move to the next page
            page += 1

            # Wait before the next API call to avoid hitting rate limits
            time.sleep(1)  # Adjust sleep time as per the API rate limit

        else:
            # If no data is returned, exit the loop
            logging.error("No data returned from API.")
            break

    # Ensure all messages are flushed and producer closes gracefully
    producer.flush()  
    producer.close(timeout=30)
    logging.info("Kafka producer closed.")
